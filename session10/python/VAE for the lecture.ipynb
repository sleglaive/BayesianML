{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tested with pytorch 1.2.0 \n",
    "\n",
    "# import matplotlib\n",
    "# matplotlib.use('Qt4Agg') # if problem with PyQt5\n",
    "import matplotlib.pyplot as plt\n",
    "my_seed = 0\n",
    "import numpy as np\n",
    "np.random.seed(my_seed)\n",
    "import torch\n",
    "torch.manual_seed(my_seed)\n",
    "import torch.nn as nn\n",
    "from utils import to_img, plot_reconstructions_VAE, display_digits\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu: True \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Using gpu: %s ' % torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative model\n",
    "\n",
    "Let $\\mathbf{x} \\in \\mathbb{R}^D$ and $\\mathbf{z} \\in \\mathbb{R}^K$ be two random vectors (with $K \\ll D$). \n",
    "\n",
    "**Generative model**: We consider the following generative model:\n",
    "\n",
    "\n",
    "$$p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$$\n",
    "\n",
    "$$p(\\mathbf{x} | \\mathbf{z} ; \\theta ) = \\mathcal{N}\\left( \\boldsymbol{\\mu}_\\theta(\\mathbf{z}), \\mathbf{I} \\right)$$\n",
    "\n",
    "The decoder outputs $\\boldsymbol{\\mu}_\\theta(\\mathbf{z})$. In theory, the mean of a Gaussian distribution lies in $]- \\infty, +\\infty[$. However, we know that our data (MNIST images) lie in $[0, 1]^D$, so we expect the mean of $p(\\mathbf{x} | \\mathbf{z} ; \\theta )$ to be in this interval too. This can be enforced by using a sigmoid activation function on the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        \n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.decoder_fc1 = nn.Linear(encoding_dim, 128)\n",
    "        self.decoder_fc2 = nn.Linear(128, 512)\n",
    "        self.decoder_output_mean = nn.Linear(512, input_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        From a latent vector, this function computes and returns \n",
    "        the mean of p(x|z).\n",
    "        \"\"\"\n",
    "    \n",
    "        z = self.relu(self.decoder_fc1(z))\n",
    "        z = self.relu(self.decoder_fc2(z))\n",
    "        z = self.sigmoid(self.decoder_output_mean(z)) \n",
    "        \n",
    "        return z\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (decoder_fc1): Linear(in_features=16, out_features=128, bias=True)\n",
      "  (decoder_fc2): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (decoder_output_mean): Linear(in_features=512, out_features=784, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoding_dim = 16\n",
    "input_dim = 784 # images of 28 x 28 pixels\n",
    "\n",
    "model = VAE(input_dim, encoding_dim)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_VAE(model, encoding_dim):\n",
    "    with torch.no_grad():\n",
    "        # sample a Gaussian random vector with zero mean and identity covariance matrix\n",
    "        z = torch.randn(1,encoding_dim).to(device)\n",
    "        # pass it through the decoder\n",
    "        x = model.decode(z).reshape(28,28).cpu()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = sample_VAE(model, encoding_dim)\n",
    "\n",
    "display_digits(x[np.newaxis, np.newaxis, :], n_i=1, n_j=1, figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference model\n",
    "\n",
    "\n",
    "**Inference model**: We consider the following inference model:\n",
    "\n",
    "$$q(\\mathbf{z} | \\mathbf{x}; \\phi) = \\mathcal{N}\\left( \\boldsymbol{\\mu}_\\phi(\\mathbf{x}), \\text{diag}\\left\\{ \\mathbf{v}_\\phi(\\mathbf{x}) \\right\\} \\right)$$\n",
    "\n",
    "\n",
    "The encoder outputs $\\boldsymbol{\\mu}_\\phi(\\mathbf{x})$ and $\\mathbf{v}_\\phi(\\mathbf{x})$. A variance has to be positive, so a common practice consists in considering that the network ouputs the logarithm of the variance. You have to properly choose the activation function of the output layer of the encoder based on this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        \n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder_fc1 = nn.Linear(input_dim, 512)\n",
    "        self.encoder_fc2 = nn.Linear(512, 128)\n",
    "        self.encoder_output_mean = nn.Linear(128, encoding_dim)\n",
    "        self.encoder_output_log_var = nn.Linear(128, encoding_dim)\n",
    "        \n",
    "        self.decoder_fc1 = nn.Linear(encoding_dim, 128)\n",
    "        self.decoder_fc2 = nn.Linear(128, 512)\n",
    "        self.decoder_output_mean = nn.Linear(512, input_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        From an input vector (MNIST image), this function computes \n",
    "        and returns the mean and log-variance of q(z|x).\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.relu(self.encoder_fc1(x))\n",
    "        x = self.relu(self.encoder_fc2(x))\n",
    "        \n",
    "        return self.encoder_output_mean(x), self.encoder_output_log_var(x)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        From a latent vector, this function computes and returns \n",
    "        the mean of p(x|z).\n",
    "        \"\"\"\n",
    "        \n",
    "        z = self.relu(self.decoder_fc1(z))\n",
    "        z = self.relu(self.decoder_fc2(z))\n",
    "        z = self.sigmoid(self.decoder_output_mean(z)) \n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almost complete VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder_fc1 = nn.Linear(input_dim, 512)\n",
    "        self.encoder_fc2 = nn.Linear(512, 128)\n",
    "        self.encoder_output_mean = nn.Linear(128, encoding_dim)\n",
    "        self.encoder_output_log_var = nn.Linear(128, encoding_dim)\n",
    "        \n",
    "        self.decoder_fc1 = nn.Linear(encoding_dim, 128)\n",
    "        self.decoder_fc2 = nn.Linear(128, 512)\n",
    "        self.decoder_output_mean = nn.Linear(512, input_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        From an input vector (MNIST image), this function computes \n",
    "        and returns the mean and log-variance of q(z|x).\n",
    "        \"\"\"\n",
    "        x = self.relu(self.encoder_fc1(x))\n",
    "        x = self.relu(self.encoder_fc2(x))\n",
    "        return self.encoder_output_mean(x), self.encoder_output_log_var(x)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        From a latent vector, this function computes and returns \n",
    "        the mean of p(x|z).\n",
    "        \"\"\"\n",
    "        z = self.relu(self.decoder_fc1(z))\n",
    "        z = self.relu(self.decoder_fc2(z))\n",
    "        z = self.sigmoid(self.decoder_output_mean(z)) \n",
    "        return z\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        From the mean and log-variance of q(z|x), this function returns a \n",
    "        latent vector drawn from q(z|x) using the reparametrization trick.\n",
    "        \"\"\"\n",
    "        ######### TO COMPLETE #########\n",
    "\n",
    "        ###############################\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        \n",
    "        return x_hat, mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the forward, between encoding and decoding, we have to sample from the inference model using the reparametrization trick. \n",
    "\n",
    "Complete the ```reparameterize``` function (3 lines of code). \n",
    "\n",
    "You can use the following PyTorch functions: [torch.randn_like](https://pytorch.org/docs/stable/torch.html#torch.randn_like) and [torch.exp](https://pytorch.org/docs/stable/torch.html#torch.exp)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
