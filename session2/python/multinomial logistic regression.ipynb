{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem formulation**\n",
    "\n",
    "Let $\\mathbf{x} \\in \\mathbb{R}^D$ denote an input vector (e.g. the image of a handwritten digit) and $y \\in \\{1,...,C\\}$ the corresponding label (e.g. the digit class).\n",
    "\n",
    "We assume that there exist a function $f(\\cdot; \\boldsymbol\\theta): \\mathbb{R}^D \\mapsto [0,1]^C$ parametrized by $\\boldsymbol\\theta \\in \\mathbb{R}^P$ such that:\n",
    "\n",
    "$$p(y=c|\\mathbf{x} ; \\theta) = f_c(\\mathbf{x}; \\boldsymbol\\theta), \\qquad \\forall c \\in \\{1,...,C\\},$$\n",
    "\n",
    "where $f_c(\\mathbf{x}; \\boldsymbol\\theta)$ is the $c$-th entry of $f(\\mathbf{x}; \\boldsymbol\\theta) \\in [0,1]^C$. \n",
    "\n",
    "Our goal is to build and train a model $f(\\mathbf{x}; \\boldsymbol\\theta)$ such that the prediction\n",
    "\n",
    "$$ \\hat{y} = \\arg\\max_{c \\in \\{1,...,C\\}} p(y=c|\\mathbf{x} ; \\theta) $$\n",
    "\n",
    "is as close as possible to the true label $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**\n",
    "\n",
    "Using the [softmax function](https://en.wikipedia.org/wiki/Softmax_function), our model $f_c(\\mathbf{x}; \\boldsymbol\\theta)$ is defined as follows:\n",
    "\n",
    "$$ f_c(\\mathbf{x}; \\boldsymbol\\theta) = \\frac{\\exp(\\boldsymbol\\theta_c^\\top \\mathbf{x} + b_c)}{ \\sum_{k=1}^C \\exp(\\boldsymbol\\theta_k^\\top \\mathbf{x} + b_k)}, $$\n",
    "\n",
    "where $\\boldsymbol\\theta = \\{\\boldsymbol\\theta_c \\in \\mathbb{R}^D, b_c \\in \\mathbb{R}\\}_{c=1}^{C}$ are the model parameters.\n",
    "\n",
    "This is the [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) model. Even though the name contains \"regression\", it is a model used for solving classification problems.\n",
    "\n",
    "Multinomial logistic regression is considered as a linear method, because the prediction is computed by a linear combination of the input features, plus a bias. The softmax function, which is indeed not linear, is only introduced to ensure that $f_c(\\mathbf{x}; \\boldsymbol\\theta) \\ge 0$ and $\\sum_{c=1}^C f_c(\\mathbf{x}; \\boldsymbol\\theta) = 1$, which are properties required to interpret the output as a probability.\n",
    "\n",
    "Note that the bias terms $b_c$ can be integrated into the weight vector $\\boldsymbol\\theta_c$ if we consider that the input vector $\\mathbf{x}$ has an additional dimension equal to 1. The model thus simplifies as:\n",
    "\n",
    "$$ f_c(\\mathbf{x}; \\boldsymbol\\theta) = \\frac{\\exp(\\boldsymbol\\theta_c^\\top \\mathbf{x})}{ \\sum_{k=1}^C \\exp(\\boldsymbol\\theta_k^\\top \\mathbf{x})}. $$\n",
    "\n",
    "In the following, we assume that the data dimension $D$ already includes this additional input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**\n",
    "\n",
    "We assume that we have access to a training dataset of $N$ i.i.d samples $ \\{(\\mathbf{x}_i, y_i) \\in \\mathbb{R}^D \\times \\{1,...,C\\}\\}_{i=1}^N$. We are in a **supervised** learning framework.\n",
    "\n",
    "The empirical risk is defined by:\n",
    "\n",
    "$$ \\mathcal{L}(\\boldsymbol\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\ell(y_i, f(\\mathbf{x}_i; \\boldsymbol\\theta)), $$\n",
    "\n",
    "where $\\ell$ is a loss function measuring the discrepancy between the true label $y_i$ and the prediction $f(\\mathbf{x}_i; \\boldsymbol\\theta)$. In multinomial logistic regression, we use the negative log-likelihood (NLL) as loss function. \n",
    "\n",
    "Let us define the one-hot vector $\\mathbf{t}_i \\in \\{0,1\\}^C$ encoding the true label $y_i$:\n",
    "\n",
    "$$ t_{i,c} = [\\mathbf{t}_i]_c = \\begin{cases} 1 \\hspace{.25cm} \\text{if } y_i =c \\\\ 0 \\hspace{.25cm} \\text{otherwise }  \\end{cases}.$$\n",
    "\n",
    "\n",
    "\n",
    "The NLL loss function is defined by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ell(y_i, f(\\mathbf{x}; \\boldsymbol\\theta)) &= - \\ln p(\\mathbf{x}_i,y_i; \\boldsymbol\\theta) \\\\\n",
    "&= - \\ln p(y_i | \\mathbf{x}_i ; \\boldsymbol\\theta) + cst(\\boldsymbol\\theta) \\\\\n",
    "&= - \\ln \\prod_{c=1}^{C} p(y_i=c | \\mathbf{x}_i ; \\boldsymbol\\theta)^{t_{i,c}} \\\\\n",
    "&= - \\ln \\prod_{c=1}^{C} f_c(\\mathbf{x}_i; \\boldsymbol\\theta)^{t_{i,c}} \\\\\n",
    "&= - \\sum_{c=1}^{C} t_{i,c} \\ln f_c(\\mathbf{x}_i; \\boldsymbol\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The complete loss function is therefore given by:\n",
    "\n",
    "$$ \\mathcal{L}(\\boldsymbol\\theta) = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{c=1}^{C} t_{i,c} \\ln f_c(\\mathbf{x}_i; \\boldsymbol\\theta). $$\n",
    "\n",
    "This is called the **cross-entropy loss**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization algorithm**\n",
    "\n",
    "To estimate the model parameters $\\boldsymbol\\theta$ we have to minimize the loss function $\\mathcal{L}(\\boldsymbol\\theta)$. To do so, we can use the [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) algorithm. It is an iterative algorithm which consists in iterating:\n",
    "\n",
    "$$ \\boldsymbol\\theta \\leftarrow \\boldsymbol\\theta - \\eta \\nabla \\mathcal{L}(\\boldsymbol\\theta), $$\n",
    "\n",
    "where $\\eta$ is the learning rate. Both the learning rate and the initialization of the parameters have a critical influence on the behavior of the algorithm.\n",
    "\n",
    "We will perform this update independently for all $\\boldsymbol\\theta_j \\in \\boldsymbol\\theta$, $j \\in \\{1,...,C\\}$, so we need to compute \n",
    "\n",
    "$$\\nabla_{\\boldsymbol\\theta_j} \\mathcal{L}(\\boldsymbol\\theta) =  - \\frac{1}{N} \\sum_{i=1}^N \\sum_{c=1}^{C} t_{i,c} \\nabla_{\\boldsymbol\\theta_j} \\ln f_c(\\mathbf{x}_i; \\boldsymbol\\theta). $$\n",
    "\n",
    "We can show that for $j=c$:\n",
    "\n",
    "$$\\nabla_{\\boldsymbol\\theta_j} \\ln f_c(\\mathbf{x}_i; \\boldsymbol\\theta) = (1 - f_c(\\mathbf{x}_i; \\boldsymbol\\theta)) \\mathbf{x}_i, $$\n",
    "\n",
    "and for $j \\neq c$:\n",
    "\n",
    "$$\\nabla_{\\boldsymbol\\theta_j} \\ln f_c(\\mathbf{x}_i; \\boldsymbol\\theta) =  - f_j(\\mathbf{x}_i; \\boldsymbol\\theta)) \\mathbf{x}_i. $$\n",
    "\n",
    "Therefore, for all $j \\in \\{1,...,C\\}$ we have:\n",
    "\n",
    "$$\\nabla_{\\boldsymbol\\theta_j} \\ln f_c(\\mathbf{x}_i; \\boldsymbol\\theta) = (t_{i,j} - f_j(\\mathbf{x}_i; \\boldsymbol\\theta)) \\mathbf{x}_i .$$\n",
    "\n",
    "**The proof is left as an exercise**\n",
    "\n",
    "Injecting this solution into the expression of $\\nabla_{\\boldsymbol\\theta_j} \\mathcal{L}(\\boldsymbol\\theta)$ we have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\boldsymbol\\theta_j} \\mathcal{L}(\\boldsymbol\\theta) &=  \\frac{1}{N} \\sum_{i=1}^N (f_j(\\mathbf{x}_i; \\boldsymbol\\theta) - t_{i,j}) \\mathbf{x}_i.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical work\n",
    "\n",
    "We start by loading a dataset of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def to_one_hot(y):\n",
    "    y_one_hot = np.zeros((y.shape[0], y.max()+1))\n",
    "    y_one_hot[np.arange(y.shape[0]),y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "def proba_to_class(y_hat):\n",
    "    return np.argmax(y_hat, axis=1)   \n",
    "\n",
    "# Each datapoint is a 8x8 image of a digit (between 0 and 9)\n",
    "x_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
    "x_digits = x_digits / x_digits.max()\n",
    "x_digits = np.hstack((x_digits, np.ones((x_digits.shape[0],1))))\n",
    "\n",
    "n_samples = len(x_digits)\n",
    "\n",
    "x_train = x_digits[:int(.9 * n_samples)] # training images\n",
    "y_train = y_digits[:int(.9 * n_samples)] # training labels\n",
    "x_test = x_digits[int(.9 * n_samples):] # testing images\n",
    "y_test = y_digits[int(.9 * n_samples):] # testing labels\n",
    "\n",
    "# we convert the labels to one-hot vectors\n",
    "y_one_hot_train = to_one_hot(y_train)\n",
    "y_one_hot_test = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_one_hot_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_one_hot_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the first 5 training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, label) in enumerate(zip(x_train[0:5,:-1], y_train[0:5])):\n",
    " plt.subplot(1, 5, index + 1)\n",
    " plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
    " plt.title('Training: %i\\n' % label, fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the following functions to understand what they implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Computes the softmax function.\n",
    "    Input: x (N, D) array\n",
    "    Returns: (N, D) array\n",
    "    \"\"\"\n",
    "    num = np.exp(x)\n",
    "    den = np.sum(num, axis=1)\n",
    "    return num/den[:,np.newaxis]\n",
    "\n",
    "def prediction(x, theta):\n",
    "    \"\"\"\n",
    "    Computes the multinomial logistic regression prediction\n",
    "    Input: x (N, D) array\n",
    "           theta (C, D) array where C is the number of classes\n",
    "    Returns: prediction (N, C) array of class probabilities\n",
    "    \"\"\"\n",
    "    return softmax(x @ theta.T) # (N, C)\n",
    "   \n",
    "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Computes the averaged cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions (N, C) array\n",
    "           targets (N, C) array        \n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.sum(targets*np.log(predictions))/N\n",
    "    return ce\n",
    "\n",
    "def cross_entropy_grad(predictions, targets, x):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the cross entropy loss\n",
    "    Input: predictions (N, C) array\n",
    "           targets (N, C) array    \n",
    "           x (N, D) array\n",
    "    Returns: gradient (C, D) array \n",
    "    \"\"\"\n",
    "    C = predictions.shape[1]\n",
    "    D = x.shape[1]\n",
    "    grad = np.zeros((C, D))\n",
    "    \n",
    "    for j in np.arange(C):\n",
    "        tmp = predictions[:,j] - targets[:,j]\n",
    "        grad[j,:] = np.mean( tmp[:, np.newaxis]*x, axis=0 )\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following cell to implement the gradient descent algorithm using the above functions. After each epoch, you will compute the cross entropy and append it to the list `ce_train`. You will also compute the accuracy using the `accuracy_score` function of scikit-learn and append it to the list `acc_train`.\n",
    "\n",
    "Run the algorithm and play with the hyperparameters (learning rate, number of iterations) to understand their influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = x_train.shape[1] # dimension of the input vector (i.e. the image of a handwritten digit) = 65\n",
    "N = x_train.shape[0] # number of training examples = 1617\n",
    "C = y_one_hot_train.shape[1] # number of classes = 10\n",
    "\n",
    "ce_train = [] \n",
    "acc_train = []\n",
    "\n",
    "theta = None # randomly initialize model parameters\n",
    "lr = None # learning rate for GD algorithm\n",
    "n_iter = None # number of iter. for GD algo.\n",
    "\n",
    "for epoch in np.arange(n_iter):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(ce_train)\n",
    "plt.title('cross entropy loss')\n",
    "plt.xlabel('iterations')\n",
    "plt.figure()\n",
    "plt.plot(acc_train)\n",
    "plt.title('accuracy')\n",
    "plt.xlabel('iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are satisfied with your performance on the training set, compute the accuracy on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the prediction on the test set\n",
    "y_prob_test_hat = prediction(x_test, theta)\n",
    "\n",
    "# compute the estimated class given the vector of probabilities\n",
    "y_test_hat = proba_to_class(y_prob_test_hat)\n",
    "\n",
    "# compute the accuracy on the test set\n",
    "acc_test = accuracy_score(y_test, y_test_hat)\n",
    "\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a validation set from the training set and find a way to use it in order to stop the gradient descent algorithm before the model overfits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
