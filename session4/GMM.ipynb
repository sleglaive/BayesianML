{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian mixture model estimation with the EM algorithm\n",
    "\n",
    "<img src=\"adventures_bayes.gif\" width=\"500\" align=\"center\">\n",
    "\n",
    "Image credit:  [Bayesian Learning for Signal Processing](https://members.loria.fr/ADeleforge/files/bayesian_inference_electronic.pdf), Antoine Deleforge, LVA/ICA 2015 Summer School."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gmm_tools import plot_GMM, plot_data, generate_Bayes_adventures_data\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Tuesday October 7, 1949, Thomas Bayes is going to visit Oxford University. Upon arriving at the university, three prankster students throw dozens of small stones at him from the roof. Bayes wants to know which student has thrown which stone. Determined, he begins to note the 2D position of each single stone on the ground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 500 # number of samples (number of stones)\n",
    "D = 2 # number of dimensions (x and y coordinates)\n",
    "\n",
    "gen = generate_Bayes_adventures_data(N, D)\n",
    "x = gen[-1]\n",
    "\n",
    "plot_data(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical work\n",
    "\n",
    "## Generative model\n",
    "\n",
    "<img src=\"bayes_latents_1.png\" width=\"400\" align=\"center\">\n",
    "\n",
    "For his investigation, Thomas Bayes defines the generative process of the observed data as follows:\n",
    "\n",
    "He observes a realization of a set of **observed random variables** denoted by $ \\mathbf{x} = \\{\\mathbf{x}_n \\in \\mathbb{R}^2\\}_{n=1}^N$, where $\\mathbf{x}_n$ corresponds to the 2D position of the $n$-th stone.\n",
    "\n",
    "These observations are generated from a set of **latent unobserved random variables** denoted by $ \\mathbf{z} = \\{z_n \\in \\{1,...,K\\} \\}_{n=1}^N$, where $z_n$ denotes the identity of the student (among $K=3$ students) who threw the $n$-th stone.\n",
    "\n",
    "The relationships between the latent and observed variables are defined by their **joint distribution**, also called **complete-data likelihood**:\n",
    "\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "p(\\mathbf{x}, \\mathbf{z}; \\theta) &= \\prod_{n=1}^N p(\\mathbf{x}_n | {z}_n; \\theta) p({z}_n; \\theta)  \\\\\n",
    "&= \\prod_{n=1}^N \\prod_{k=1}^K \\left( p(\\mathbf{x}_n | {z}_n=k; \\theta) p({z}_n=k; \\theta) \\right)^{\\mathbb{1}\\{z_n = k\\}},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathbb{1}\\{z_n = k\\} = \\begin{cases}1 & \\text{if } z_n = k \\\\ 0 & \\text{otherwise}\\end{cases}$.\n",
    "\n",
    "\n",
    "The **prior** over the latent variables follows a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution):\n",
    "$$ p({z}_n=k; \\theta) = \\pi_k, \\qquad k \\in \\{1,...,K\\}, \\qquad \\text{with }\\, \\pi_k > 0\\, \\text{ and }\\, \\sum_{k=1}^K = 1. $$\n",
    "\n",
    "The **likelihood** is [Gaussian](https://en.wikipedia.org/wiki/Multivariate_normal_distribution):\n",
    "\n",
    "$$ p(\\mathbf{x}_n | z_n=k; \\theta) = \\mathcal{N}(\\mathbf{x}_n; \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k),$$\n",
    "\n",
    "with $\\mathcal{N}(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\displaystyle \\frac{1}{\\sqrt{\\det(2\\pi \\boldsymbol\\Sigma)}} \\exp\\left(-\\frac 1 2 ({\\mathbf x}-{\\boldsymbol\\mu})^\\mathrm{T}{\\boldsymbol\\Sigma}^{-1}({\\mathbf x}-{\\boldsymbol\\mu})\\right).$\n",
    "\n",
    "The set of **unknown deterministic model parameters** is defined by:\n",
    "\n",
    "$$ \\theta = \\{\\pi_k, \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k\\}_{k=1}^K. $$\n",
    "\n",
    "The **complete-data log-likelihood** is therefore given by:\n",
    "$$ \\ln p(\\mathbf{x}, \\mathbf{z}; \\theta) =  \\sum_{n=1}^N \\sum_{k=1}^K \\mathbb{1}\\{z_n = k\\} \\left(\\ln \\pi_k + \\ln \\mathcal{N}(\\mathbf{x}_n; \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\right). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior inference\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "#### Question 1.1 \n",
    "\n",
    "Give the expression of the responsabilities $ r_{n,k} \\triangleq p(z_n = k | \\mathbf{x}_n; \\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.2 \n",
    "\n",
    "How can you interpret the responsabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.3 \n",
    "\n",
    "In order to compute the responsabilities, it is necessary to estimate the unknown model parameters $\\theta$. To do so, we would like to maximize the log-marginal likelihood $\\ln p(\\mathbf{x}; \\theta) $. Give its expression and explain why it cannot be directly optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation-Maximization algorithm\n",
    "\n",
    "As direct maximum log-marginal likelihood estimation is intractable, we will derive an expectation-maximization (EM) algorithm.\n",
    "\n",
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.1 \n",
    "\n",
    "Let $\\tilde{\\theta}$ denote the current estimate of the model parameters. Using the above definition of the complete-data log-likelihood, solve the E-step, that is compute the so-called $Q$-function, defined by: \n",
    "\n",
    "$$\\begin{aligned}\n",
    "Q(\\theta, \\tilde{\\theta}) &= \\mathbb{E}_{p(\\mathbf{z} | \\mathbf{x}; \\tilde{\\theta})}[\\ln p(\\mathbf{x}, \\mathbf{z}; \\theta)] \\end{aligned}$$\n",
    "\n",
    "Make the depency on the model parameters $\\theta = \\{\\pi_k, \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k\\}_{k=1}^K$ explicit (any constant with respect to these parameters can be omitted).\n",
    "\n",
    "**Hints:**\n",
    "- The expectation of a sum is the sum of the expectations.\n",
    "- $\\mathbb{E}_{p(\\mathbf{z} | \\mathbf{x}; \\tilde{\\theta})}[f(\\mathbf{z}_n)] = \\mathbb{E}_{p(\\mathbf{z}_n | \\mathbf{x}_n; \\tilde{\\theta})}[f(\\mathbf{z}_n)]$ for any arbitrary function $f$;\n",
    "- $\\mathbb{E}_{p(\\mathbf{z}_n | \\mathbf{x}_n; \\tilde{\\theta})}[\\mathbb{1}\\{z_n = k\\}] = \\sum\\limits_{z_n = 1}^K \\mathbb{1}\\{z_n = k\\} p(\\mathbf{z}_n | \\mathbf{x}_n; \\tilde{\\theta}) = p(\\mathbf{z}_n = k | \\mathbf{x}_n; \\tilde{\\theta}) = r_{n,k} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2 \n",
    "\n",
    "You now have to solve the M-step, that is updating the model parameters by maximizing $Q(\\theta, \\tilde{\\theta})$ with respect to (w.r.t) $\\theta$. To do so, you will simply cancel the partial derivatives of $Q(\\theta, \\tilde{\\theta})$ w.r.t $\\boldsymbol{\\mu}_k$, $\\boldsymbol{\\Sigma}_k$ and $\\pi_k$.\n",
    "\n",
    "Useful matrix derivation formulas can be found in the appendix at the end of this notebook, or in the [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2a \n",
    "\n",
    "Compute the partial derivative of $Q(\\theta, \\tilde{\\theta})$ w.r.t $\\boldsymbol{\\mu}_k$ and set it to zero to get the update of $\\boldsymbol{\\mu}_k$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_{\\boldsymbol{\\mu}_k} Q(\\theta, \\tilde{\\theta}) &=  \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "You will express the update as a function of $N_k = \\sum_{n=1}^N r_{n,k}$. If we interpret $r_{n,k}$ as being equal to 1 if $\\mathbf{x}_n$ belongs to component $k$ and 0 otherwise, $N_k$ corresponds to the number of points assigned to cluster $k$.\n",
    "\n",
    "**Hint**: $\\boldsymbol{\\Sigma}_k$ is a covariance matrix so it is symetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2b\n",
    "\n",
    "Compute the partial derivative of $Q(\\theta, \\tilde{\\theta})$ w.r.t $\\boldsymbol{\\Sigma}_k$ and set it to zero to get the update of $\\boldsymbol{\\Sigma}_k$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_{\\boldsymbol{\\Sigma}_k} Q(\\theta, \\tilde{\\theta}) &=  \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "You will express the update as a function of $N_k = \\sum_{n=1}^N r_{n,k}$.\n",
    "\n",
    "**Hint**: Use the [trace trick](https://math.stackexchange.com/questions/1761198/proof-on-trace-trick): $\\mathbf{x}^\\top\\boldsymbol{\\Sigma}^{-1}\\mathbf{x} = tr(\\mathbf{x}^\\top\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}) = tr(\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}\\mathbf{x}^\\top) $ and then refer to the matrix derivation formulas in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2c\n",
    "\n",
    "The update for $\\pi_k$ is obtained by maximizing $Q(\\theta, \\tilde{\\theta})$ under the constraint that $\\sum_{k=1}^K \\pi_k = 1$. We obtain:\n",
    "\n",
    "$$ \\pi_k = N_k / N, $$\n",
    "\n",
    "where $N_k = \\sum_{n=1}^N r_{n,k}$. The optimal prior probablity $p(z_n = k) = \\pi_k$ is thus given by the number of points $N_k$ in cluster $k$ divided by the total number of points $N$.\n",
    "\n",
    "To obtain this expression you have to use the method of [Lagrange multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier): \n",
    "\n",
    "- you first cancel the partial derivative of the following Lagrangian w.r.t $\\pi_k$:\n",
    "\n",
    "    $$ \\mathcal{L}(\\theta, \\tilde{\\theta}, \\lambda) = Q(\\theta, \\tilde{\\theta}) + \\lambda \\left(\\sum_{k=1}^K \\pi_k - 1\\right). $$\n",
    "\n",
    "- then you simply inject this solution into the constraint to find out the solution for $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM():\n",
    "    \"\"\"\n",
    "    Gaussian mixture model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_comp, data_dim=2, seed=None):\n",
    "\n",
    "        super(GMM, self).__init__()\n",
    "\n",
    "        self.n_comp = n_comp\n",
    "        self.data_dim = 2\n",
    "        self.init_param(seed=seed)\n",
    "                \n",
    "    def init_param(self, pis=None, means=None, covars=None, seed=None):\n",
    "        \"\"\"\n",
    "        Initialize the model parameters using the provided arguments \n",
    "        or randomly.\n",
    "        \n",
    "        Inputs \n",
    "            pis: list of prior probabilities, length equal to self.n_comp\n",
    "            means: list of GMM means, length equal to self.n_comp\n",
    "            covars: list of GMM means, length equal to self.n_comp\n",
    "        Outputs\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        if pis is not None:\n",
    "            self.pis = pis\n",
    "        else:\n",
    "            self.pis = []\n",
    "            for k in np.arange(self.n_comp):\n",
    "                # prior set to 1/K\n",
    "                self.pis.append(1/self.n_comp)\n",
    "        \n",
    "        if means is not None:\n",
    "            self.means = means\n",
    "        else:\n",
    "            self.means = []\n",
    "            for k in np.arange(self.n_comp):\n",
    "                # mean vector drawn from a centered unit Gaussian\n",
    "                mean = np.random.randn(self.data_dim)\n",
    "                self.means.append(mean)\n",
    "                \n",
    "        if covars is not None:\n",
    "            self.covars = covars\n",
    "        else:\n",
    "            self.covars = []\n",
    "            for k in np.arange(self.n_comp):\n",
    "                # identity covariance\n",
    "                covar = np.eye(self.data_dim)\n",
    "                self.covars.append(covar)\n",
    "                \n",
    "        if seed is not None:\n",
    "            np.random.seed()\n",
    "                \n",
    "    def fit(self, data, n_iter=50):\n",
    "        \"\"\"\n",
    "        Fit a GMM with the EM algorithm\n",
    "        \n",
    "        Inputs \n",
    "            data (number of points, dimension) array\n",
    "            n_iter \n",
    "               \n",
    "        Outputs\n",
    "            log-marginal likelihood\n",
    "        \"\"\"\n",
    "        LML = []\n",
    "              \n",
    "        for iter in np.arange(n_iter):\n",
    "            \n",
    "            resp = self.E_step(data)\n",
    "            self.M_step(data, resp)\n",
    "            LML.append(self.compute_LML(data))\n",
    "            \n",
    "        return LML\n",
    "    \n",
    "    def E_step(self, data):\n",
    "        \"\"\"\n",
    "        Compute the responsabilities\n",
    "        \n",
    "        Inputs \n",
    "            data (number of points, dimension) array\n",
    "               \n",
    "        Outputs\n",
    "            responsabilities (number of points, number of GMM components)\n",
    "        \"\"\"\n",
    "        \n",
    "        N = data.shape[0]\n",
    "        \n",
    "        resp = np.zeros((N,self.n_comp))\n",
    "        \n",
    "        ###### TO COMPLETE ######\n",
    "        # Use the static method GMM.compute_pdf_multi_gaussian() defined below\n",
    "        \n",
    "        #########################\n",
    "        \n",
    "        return resp\n",
    "\n",
    "    def M_step(self, data, resp):\n",
    "        \"\"\"\n",
    "        Update the model parameters\n",
    "        \n",
    "        Inputs \n",
    "            data: (number of points, dimension) array\n",
    "               \n",
    "        Outputs\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        ###### TO COMPLETE ######\n",
    "        pass\n",
    "        #########################\n",
    "            \n",
    "    def compute_LML(self, data):\n",
    "        \"\"\"\n",
    "        Compute the log-marginal likelihood\n",
    "        \n",
    "        Inputs \n",
    "            data: (number of points, dimension) array\n",
    "               \n",
    "        Outputs\n",
    "            log-marginal likelihood\n",
    "        \"\"\"\n",
    "        \n",
    "        LML = 0\n",
    "        \n",
    "        ###### TO COMPLETE ######              \n",
    "        \n",
    "        #########################\n",
    "        \n",
    "        return LML\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_pdf_multi_gaussian(data, mean, covar):\n",
    "        \"\"\"\n",
    "        Compute the pdf of a multivariate Gaussian distribution\n",
    "                \n",
    "        Inputs \n",
    "            data: data points to evaluate the pdf (number of points, dimension) array\n",
    "            mean: mean vector (dimension,) array\n",
    "            covar: covariance matrix (dimension, dimension) array\n",
    "               \n",
    "        Outputs\n",
    "            pdf evaluated on 'data', (number of points,) array\n",
    "        \"\"\"\n",
    "        rv = multivariate_normal(mean, covar)\n",
    "        return rv.pdf(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```GMM``` class defined in the previous cell implements a Gaussian mixture model. It has two important methods:\n",
    "- ```init_param()``` initializes the model parameters\n",
    "- ```fit()``` runs the EM algorithm to estimate the model parameters. It alternates between the E- and M-steps, and after each iteration it computes the log-marginal likelihood.\n",
    "\n",
    "In the following cell, we instantiate this class for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GMM(n_comp=3, data_dim=2, seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "#### Exercise 3.1\n",
    "\n",
    "Complete the method that computes the log-marginal likelihood (LML) and run the following cell.\n",
    "\n",
    "The LML is defined as a sum over the data points. You will divide this sum by the number of data points, so that the value of the objective function does not depend on the size of the dataset. In other words, compute the mean instead of the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LML_init = gmm.compute_LML(x)\n",
    "\n",
    "print(\"log-marginal likelihood: %.4f\" % LML_init)\n",
    "\n",
    "if int(LML_init*1000) == -22548:\n",
    "    print(\"so far, it seems to be ok\")\n",
    "else:\n",
    "    print(\"argh, this is not the expected result, either you made a mistake, or my unit test is badly designed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2\n",
    "\n",
    "Complete the method that computes the E-step and run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = gmm.E_step(x)\n",
    "if np.sum(resp) == N:\n",
    "    print(\"so far, it seems to be ok\")\n",
    "else:\n",
    "    print(\"argh, this is not the expected result, either you made a mistake, or my unit test is badly designed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assign each point to each cluster, we simply look at the argmax of the reponsabilities. Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hat = np.argmax(resp, axis=1)\n",
    "\n",
    "fig1 = plt.figure(figsize=(10,4))\n",
    "ax1 = fig1.add_subplot(111)\n",
    "plot_GMM(x, z_hat, gmm.means, gmm.covars, colors=['b','g','r'], ax=ax1)\n",
    "ax1.set_title('estimation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explain what you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.3\n",
    "\n",
    "Complete the method that computes the M-step and run the following cell.\n",
    "\n",
    "Hint: Updating the covariance matrix requires computing the outer product of vectors. Look at the notebook `numpy new axis trick` to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm.M_step(x, resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LML = gmm.compute_LML(x)\n",
    "delta_LML = LML - LML_init\n",
    "print(\"log-marginal likelihood: %.4f\" % LML)\n",
    "print(\"log-marginal likelihood improvement: %.4f\" % delta_LML)\n",
    "\n",
    "if int(delta_LML*1000) == 19556:\n",
    "    print(\"\\nthe log-marginal likelihood increased, well done!\")\n",
    "else:\n",
    "    print(\"argh, this is not the expected result, either you made a mistake, or my unit test is badly designed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you got all my encouraging messages, then you are ready to fit the GMM on the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LML = gmm.fit(data=x, n_iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we plot the log-marginal likelihood along the iterations. It should be monotonically increasing, a nice feature of the EM algorithm which is very useful for debugging: if the log-marginal likelihood decreases, there is a bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LML)\n",
    "plt.title(\"log-marginal likelihood\")\n",
    "plt.xlabel(\"EM iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = gmm.E_step(x)\n",
    "z_hat = np.argmax(resp, axis=1)\n",
    "\n",
    "fig1 = plt.figure(figsize=(10,4))\n",
    "ax1 = fig1.add_subplot(111)\n",
    "plot_GMM(x, z_hat, gmm.means, gmm.covars, colors=['b','g','r'], ax=ax1)\n",
    "ax1.set_title('estimation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used synthetic data, so we actually also know the true model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pis_true, means_true, covars_true, z_true, _) = gen\n",
    "\n",
    "\n",
    "fig2 = plt.figure(figsize=(10,4))\n",
    "ax2 = fig2.add_subplot(111)\n",
    "plot_GMM(x, z_true, means_true, covars_true, colors=['b','g','r'], ax=ax2)\n",
    "ax2.set_title('ground truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not perfect, but not that bad either... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.4\n",
    "\n",
    "Re-run the complete pipeline several times after changing the seed that is used to instantiate the GMM. Explain what you observe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Use the [K-means](https://en.wikipedia.org/wiki/K-means_clustering) algorithm to initialize the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "\n",
    "\n",
    "For $f:  \\mathbb{R}^{I \\times J} \\mapsto \\mathbb{R}$, the gradient is defined by $\\frac{d}{d \\mathbf{X}} f(\\mathbf{X}) = \\nabla_{\\mathbf{X}}f(\\mathbf{X}) =  [\\frac{\\partial}{\\partial X_{ij}} f(\\mathbf{X}) ]_{ij} $.\n",
    "\n",
    "Below are some useful derivatives:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\mathbf{x}^T \\mathbf{a}}{\\partial \\mathbf{x}} = \\frac{\\partial \\mathbf{a}^T \\mathbf{x}}{\\partial \\mathbf{x}} = \\mathbf{a}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\mathbf{x}^T \\mathbf{A} \\mathbf{x}}{\\partial \\mathbf{x}} = 2 \\mathbf{A} \\mathbf{x}, \\qquad \\text{if } \\mathbf{A} \\text{ is symmetric}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\mathbf{X}}tr(\\mathbf{A}\\mathbf{X}^T) = \\mathbf{A}\n",
    "\\label{derTrace1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\mathbf{X}}tr(\\mathbf{A}\\mathbf{X}) = \\mathbf{A}^T\n",
    "\\label{derTrace2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\mathbf{X}}tr(\\mathbf{X}^{-1}\\mathbf{A}) = -(\\mathbf{X}^{-1}\\mathbf{A}\\mathbf{X}^{-1})^T\n",
    "\\label{derTraceInverse}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\mathbf{X}}\\ln \\det(\\mathbf{X}) = \\big((\\mathbf{X}^T)^{-1}\\big)^T\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
