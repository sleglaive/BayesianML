{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian model with latent mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this cell, we are just defining a plotting function\n",
    "\n",
    "def plot_pdf(data, mu_prior=None, sigma2_prior=None, mu_post=None, sigma2_post=None):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.plot(data, np.zeros_like(data), 'x', markersize=12, label='observations')\n",
    "    mu_ML = np.mean(data)\n",
    "    ax.plot(mu_ML, 0, 'og', fillstyle='none', markersize=12, label='ML estimate')\n",
    "    \n",
    "    mu_max = max(5*np.sqrt(sigma2_prior), np.abs(mu_ML))*1.1\n",
    "    mu = np.linspace(-mu_max, mu_max, 500)\n",
    "    \n",
    "    if mu_prior is not None and sigma2_prior is not None:\n",
    "        \n",
    "        ax.plot(mu, norm.pdf(mu, mu_prior, sigma2_prior), 'b-', lw=2, alpha=1, label='prior')\n",
    "    \n",
    "    if mu_post is not None and sigma2_post is not None:\n",
    "                \n",
    "        ax.plot(mu, norm.pdf(mu, mu_post, sigma2_post), 'r-', lw=2, alpha=1, label='posterior')\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    # sort both labels and handles by labels\n",
    "    labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "    ax.legend(handles[::-1], labels[::-1], fontsize=14)\n",
    "\n",
    "    plt.xlabel('$\\mu$', fontsize=15)\n",
    "    plt.ylabel('pdf', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first generate some synthetic Gaussian data. This is a toy example, for real-life problems we have no clue about the true distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.8 # mean of the true data distribution\n",
    "sigma2 = 0.1 # variance of the true data distribution\n",
    "\n",
    "N = 1 # number of observations\n",
    "\n",
    "data = mu + np.sqrt(sigma2)*np.random.randn(N) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume a Gaussian observation or likelihood model (which exactly matches with the true distribution of the data, but in general this is not the case, it is only a model). The variance of this observation model is supposed to be known, and the mean is considered as a latent random variable, equipped with a Gaussian prior whose parameters are defined in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_0 = 0\n",
    "sigma2_0 = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the prior, the observations and the ML estimate of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(data, mu_prior=mu_0, sigma2_prior=sigma2_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With N=1, the ML estimate is equal to the single observed point and is pretty far from the true value of mean, which was 0.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we compute the parameters of the Gaussian posterior distribution of the latent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML estimate\n",
    "mu_ML = np.mean(data)\n",
    "\n",
    "# posterior\n",
    "mu_post = sigma2/(N*sigma2_0 + sigma2)*mu_0 + N*sigma2_0/(N*sigma2_0 + sigma2)*mu_ML\n",
    "sigma2_post = 1/(1/sigma2_0 + N/sigma2)\n",
    "\n",
    "print(\"true mean: %.2f - ML estimate: %.2f - MAP estimate: %.2f\" % (mu, mu_ML, mu_post))\n",
    "\n",
    "plot_pdf(data, mu_prior=mu_0, sigma2_prior=sigma2_0, mu_post=mu_post, sigma2_post=sigma2_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still with N=1, you should observe that the posterior mean of the mean (hum...), which is also the maximum a posteriori (MAP), is closer to the true value than the ML estimate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "1. How does the posterior mean and variance vary if the number of observations N increases (N = 5, 10, 100)?\n",
    "\n",
    "2. What happens to the posterior (mean and variance) if you set a high value for the variance of the prior (e.g. $\\sigma_0^2 = 5$ and N=1)?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
