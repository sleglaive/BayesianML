{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian model with latent mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this cell, we are just defining a plotting function\n",
    "\n",
    "def plot_pdf(data, mu_prior=None, sigma2_prior=None, mu_post=None, sigma2_post=None):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.plot(data, np.zeros_like(data), 'x', markersize=12, label='observations')\n",
    "    mu_ML = np.mean(data)\n",
    "    ax.plot(mu_ML, 0, 'og', fillstyle='none', markersize=12, label='ML estimate')\n",
    "    \n",
    "    mu_max = max(5*np.sqrt(sigma2_prior), np.abs(mu_ML))*1.1\n",
    "    mu = np.linspace(-mu_max, mu_max, 500)\n",
    "    \n",
    "    if mu_prior is not None and sigma2_prior is not None:\n",
    "        \n",
    "        ax.plot(mu, norm.pdf(mu, mu_prior, sigma2_prior), 'b-', lw=2, alpha=1, label='prior')\n",
    "    \n",
    "    if mu_post is not None and sigma2_post is not None:\n",
    "                \n",
    "        ax.plot(mu, norm.pdf(mu, mu_post, sigma2_post), 'r-', lw=2, alpha=1, label='posterior')\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    # sort both labels and handles by labels\n",
    "    labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "    ax.legend(handles[::-1], labels[::-1], fontsize=14)\n",
    "\n",
    "    plt.xlabel('$\\mu$', fontsize=15)\n",
    "    plt.ylabel('pdf', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first generate some synthetic Gaussian data. \n",
    "\n",
    "Note that for real-world problems we have no clue about the true underlying distribution of the observed data. For instance, if your dataset is made of natural images of 1M pixels, the dimension of your data $10^6$ and they are very likely to follow a distribution which is much more complex than a Gaussian in dimension $10^6$.\n",
    "\n",
    "Anyway, we have to assume a model distribution, whcih may be Gaussian, this is the likelihood!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.8 # mean of the true data distribution\n",
    "sigma2 = 0.1 # variance of the true data distribution\n",
    "\n",
    "N = 1 # number of observations\n",
    "\n",
    "data = mu + np.sqrt(sigma2)*np.random.randn(N) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above variance is supposed to be known, while the mean is considered as a latent random variable, equipped with a Gaussian prior whose parameters are defined in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_0 = 0\n",
    "sigma2_0 = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the prior, the observations and the ML estimate of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(data, mu_prior=mu_0, sigma2_prior=sigma2_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With N=1, the ML estimate is equal to the single observed point and is pretty far from the true value of mean, which was 0.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "1. Derive the analytical expression of the posterior distribution of the mean (cf. course slides)\n",
    "2. Complete the following cell to compute the parameters of the posterior (mean and variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML estimate\n",
    "mu_ML = np.mean(data)\n",
    "\n",
    "# posterior\n",
    "mu_post = \n",
    "sigma2_post = \n",
    "\n",
    "print(\"true mean: %.2f - ML estimate: %.2f - MAP estimate: %.2f\" % (mu, mu_ML, mu_post))\n",
    "\n",
    "plot_pdf(data, mu_prior=mu_0, sigma2_prior=sigma2_0, mu_post=mu_post, sigma2_post=sigma2_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still with N=1, you should observe that the posterior mean of the mean (hum...), which is also the maximum a posteriori (MAP), is closer to the true value than the ML estimate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "1. How does the posterior mean and variance vary if the number of observations N increases (N = 5, 10, 100)?\n",
    "\n",
    "2. What happens to the posterior (mean and variance) if you set a high value for the variance of the prior (e.g. $\\sigma_0^2 = 5$ and N=1)?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
